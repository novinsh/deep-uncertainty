{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/bin/python \n",
    "import tensorflow as tf \n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, Activation, Lambda, RepeatVector, Conv2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "import keras.utils\n",
    "from keras.optimizers import SGD\n",
    "#from bnn.loss_equations import bayesian_categorical_crossentropy\n",
    "from Datas import Initialize_data,ComputeAccuracy,category_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (trx,trY,trsy,valx,valY,valy,testx,testY,testy) = Initialize_data(1)      \n",
    "# #### the dropout rate\n",
    "# rate = 0.1\n",
    "# #### build model\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(BatchNormalization())\n",
    "# first_layer = model.add(Dense(500, input_shape=(3072,), activation=\"relu\"))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(rate, noise_shape=None, seed=None))\n",
    "\n",
    "# model.add(Dense(100, activation=\"relu\",name = 'ab'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(rate, noise_shape=None, seed=None))\n",
    "\n",
    "# x = model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "\n",
    "# #### model compile\n",
    "# INIT_LR = 0.1\n",
    "# opt = SGD(lr = INIT_LR)\n",
    "# model.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
    "# EPOCHS = 10\n",
    "# BATCH = 2000\n",
    "# #### model fit\n",
    "\n",
    "# H = model.fit(trx, trY, validation_data=(valx, valY),\n",
    "#               epochs = EPOCHS, batch_size = BATCH)\n",
    "# #### model predict\n",
    "\n",
    "# predictions = model.predict(testx, batch_size = BATCH)\n",
    "# print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### compute the accuracy in test data\n",
    "def ComputeAccuracy(predictions,testY):\n",
    "    answers = predictions.argmax(axis=1)\n",
    "    (d, ) = np.shape(testY)\n",
    "    ii = np.reshape(np.array(testY),(d,1))\n",
    "    y = np.reshape(np.array(answers),(d,1))\t\t\n",
    "    correct = ii - y\n",
    "    correct[correct!=0] = -1\n",
    "    correct[correct==0] = 1\n",
    "    correct[correct==-1] = 0\n",
    "    a = np.count_nonzero(correct)\n",
    "    result = float(a)/float(d)\n",
    "    return result\n",
    "\n",
    "def Draw_loss(epochs,predictions,H):\n",
    "    N = np.arange(0, epochs)\n",
    "    plt.style.use(\"ggplot\")\n",
    "    #### print accuracy\n",
    "    plt.plot(N, H.history[\"acc\"], label=\"train_acc\")\n",
    "    plt.plot(N, H.history[\"val_acc\"], label=\"val_acc\")\n",
    "    plt.title(\" Accuracy\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    #### print loss\n",
    "    plt.figure()\n",
    "    plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.title(\"Training Loss \")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "# Draw_loss(EPOCHS,predictions,H)  \n",
    "# print('accuracy in test data')\n",
    "# print(ComputeAccuracy(predictions,testy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 2 arrays: [array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 1.],\n       [0., 0., 0., ..., 0., 0., 1.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 1.],\n       [0., 0., 0., ..., 0., 0., 0....",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-97aeedb42bb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mBatch_size\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;36m2500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mEpoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBuild_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEpoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-97aeedb42bb4>\u001b[0m in \u001b[0;36mBuild_Model\u001b[0;34m(full, Batch_size, Epoch)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     H = model.fit(trx, [trY,trY], validation_data=(valx, [valY,valY]),\n\u001b[0;32m---> 25\u001b[0;31m                     epochs = Epoch, batch_size = Batch_size)\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;31m#### model predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 2 arrays: [array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 1.],\n       [0., 0., 0., ..., 0., 0., 1.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 1.],\n       [0., 0., 0., ..., 0., 0., 0...."
     ]
    }
   ],
   "source": [
    "def Build_Model(full,Batch_size,Epoch):\n",
    "    (trx,trY,trsy,valx,valY,valy,testx,testY,testy) = Initialize_data(full)  \n",
    "    rate = 0.5\n",
    "    Inputs = Input(shape=(3072,))\n",
    "    x = BatchNormalization()(Inputs)\n",
    "    x = Dense(500, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(rate,noise_shape=None, seed=None)(x)\n",
    "    x = Dense(200, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(rate,noise_shape=None, seed=None)(x)\n",
    "    # x = Dense(50, activation=\"relu\")(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    # x = Dropout(0.5,noise_shape=None, seed=None)(x)\n",
    "    out = Dense(10, activation='softmax')(x)\n",
    "    model = Model(inputs = Inputs, outputs = out)\n",
    "\n",
    "    INIT_LR = 0.1\n",
    "    opt = SGD(lr = INIT_LR)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=[\"accuracy\"])\n",
    "\n",
    "    #### model fit\n",
    "\n",
    "    H = model.fit(trx, [trY,trY], validation_data=(valx, [valY,valY]),\n",
    "                    epochs = Epoch, batch_size = Batch_size)\n",
    "    #### model predict\n",
    "\n",
    "    predictions = model.predict(testx, batch_size = Batch_size)\n",
    "\n",
    "    #### print Accuracy \n",
    "    print('accuracy in test data')\n",
    "    print(ComputeAccuracy(predictions,testy))\n",
    "    #### Draw loss\n",
    "    Draw_loss(Epoch,predictions,H)  \n",
    "    #### calculate the uncertainty\n",
    "    #### variance ratio for test data\n",
    "    print(\"variance ratio:\")\n",
    "    print(variance_ratio(testy))\n",
    "    #### Epistemic uncertainty\n",
    "    print('Epistemic Uncertainty: predictive Entropy')\n",
    "    print(predictive_uncertainty(predictions))\n",
    "    print('Epistemic Uncertainty: Monte_carlo')\n",
    "    (prediction_probabilities, prediction_variances) = montecarlo_prediction(predictions)\n",
    "    print('prediction probabilities:')\n",
    "    print(prediction_probabilities)\n",
    "    print('predoction_variances:')\n",
    "    print(prediction_variances)\n",
    "    return predictions\n",
    "    \n",
    "full = 1\n",
    "Batch_size =  2500\n",
    "Epoch = 40\n",
    "predictions = Build_Model(full,Batch_size,Epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#aleatoric uncertainty\n",
    "\n",
    "\n",
    "-> Aleatoric uncertainty measures what you can't understand from the data\n",
    "heteroscedastic here.\n",
    "-> Aleatoric uncertainty is a function of the input data. the loss function creates a normal distribution with a mean of zero and the predicted variance. It distorts the predicted logit values by sampling from the distribution and computes the softmax categorical cross entropy using the distorted predictions. The loss function runs T Monte Carlo samples and then takes the average of the T samples as the loss.\n",
    "-> noise on data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "def variance_ratio(trainy):\n",
    "    T = len(trainy)\n",
    "    trainy = np.array(trainy)\n",
    "    ans = np.bincount(trainy).argmax()\n",
    "    \n",
    "    fx = len(trainy[np.where(trainy==ans)])\n",
    "    \n",
    "    ratio = 1- float(fx)/float(T)\n",
    "    \n",
    "    return ratio\n",
    "#(trx,trY,trsy,valx,valY,valy,testx,testY,testy) = Initialize_data(full) \n",
    "print(variance_ratio(testy))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cross_entropy(pred,true):\n",
    "#     (D,h) = softmax_data.shape\n",
    "#     print(np.sum(pred * np.log(pred), axis=1))\n",
    "    return -np.sum(true * np.log(pred), axis=1)\n",
    "def gaussian_distort(softmax_data,variance,std):\n",
    "    import scipy.stats\n",
    "    dist    = scipy.stats.norm(std,variance) \n",
    "    asx     = dist.pdf(softmax_data)\n",
    "    #print(np.array(asx).shape)\n",
    "    return np.array(asx)\n",
    "def distored_lossfn(softmax_data,variance,std,D,trainY):\n",
    "    temp_asx = np.zeros(softmax_data.shape)\n",
    "    (D,c) = softmax_data.shape\n",
    "    for i in range(D):\n",
    "        temp_asx[i,:] = gaussian_distort(softmax_data[i,:],variance[i],std[i])\n",
    "    softmax_data = softmax_data + temp_asx\n",
    "        \n",
    "#     std    = np.reshape(std,(D,1))\n",
    "    \n",
    "#     repstd = np.tile(std,(1,10))\n",
    "#     foo    = softmax_data + std\n",
    "#     repvar = np.tile(variance,(10,1))\n",
    "#     foo    = foo*2/(2*(repvar.T**2))\n",
    "    #foo    = 1/(np.sqrt(2*np.pi*(repvar.T**2))) * np.exp(-foo)\n",
    "#     print(foo)\n",
    "    return (softmax_data,temp_asx)\n",
    "def variance_std (softmax_data):\n",
    "    (D,h)              = softmax_data.shape\n",
    "    variance           = np.var(softmax_data,axis=1)\n",
    "    std                = np.sqrt(variance)  \n",
    "    return (variance,std)\n",
    "def bayesian_categorical_crossentropy_nodist(softmax_data,trainY):\n",
    "    \n",
    "    (D,h)              = softmax_data.shape\n",
    "    (variance,std) = variance_std(softmax_data)\n",
    "    variance_depressor = np.exp(variance) - np.ones_like(variance)\n",
    "    alpha = 0.1\n",
    "#     #### monte carlo results\n",
    "    undistorted_loss              = cross_entropy(softmax_data,trainY)\n",
    "    temp_asx = np.zeros(softmax_data.shape)\n",
    "    softmax_temp = softmax_data\n",
    "    for i in range(4):\n",
    "        (variance,std) = variance_std(softmax_data)\n",
    "        (softmax_temp,temp_asx)   = distored_lossfn(softmax_temp,variance,std,D,trainY)\n",
    "        \n",
    "                       \n",
    "    temp_asx     /=4\n",
    "    softmax_temp                  = softmax_data+temp_asx\n",
    "    distorted_loss                = cross_entropy(softmax_temp,trainY)\n",
    "    diff                          = undistorted_loss - distorted_loss\n",
    "    \n",
    "   \n",
    "    mask1                         = diff\n",
    "    mask1                         = np.array(mask1)\n",
    "    diff                          = np.array(diff)\n",
    "    mask1[mask1>0]                = 0\n",
    "    mask1[mask1<=0]               = 1\n",
    "    w                             = (np.exp(diff)-1)*alpha\n",
    "    w                             = w * mask1\n",
    "    diff[diff<0]                  = 0\n",
    "    diff                          = -(diff + w)\n",
    "    \n",
    "    variance_loss = np.mean(diff, axis=0) * undistorted_loss\n",
    "    \n",
    "    loss = variance_loss + undistorted_loss + variance_depressor\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# loss = bayesian_categorical_crossentropy_nodist(predictions,testY)\n",
    "# print(loss)\n",
    "#### Epistemic uncertainty\n",
    "def predictive_uncertainty(softmax_data):\n",
    "    w = softmax_data * np.log(softmax_data)\n",
    "    w = np.sum(w,axis = 1)\n",
    "   \n",
    "    return -w\n",
    "# print('predictive')\n",
    "# print(predictive_uncertainty(predictions))\n",
    "def  montecarlo_prediction(predictions):\n",
    "    prediction_probabilities = np.mean(predictions, axis=0)\n",
    "    prediction_variances = predictive_uncertainty(predictions)\n",
    "    print(prediction_variances)\n",
    "    print(prediction_probabilities)\n",
    "    return (prediction_probabilities, prediction_variances)\n",
    "\n",
    "# print('epistemic')\n",
    "# montecarlo_prediction(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### epistemic uncertainty.\n",
    "-> predictive entropy\n",
    "-> Epistemic uncertainty measures what your model doesn't know due to lack of training data. It can be explained away with infinite training data. Think of epistemic uncertainty as model uncertainty.  this should decrease during training !! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictive_entropy(pred):\n",
    "    #### for each class\n",
    "    return -1 * np.sum(np.log(pred) * pred, axis=0)\n",
    "pred_entropy = predictive_entropy(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct uncertainty:\n",
      "3521.8506850600243\n",
      "not correct uncertainty:\n",
      "10302.442619264126\n",
      "correct uncertainty each class:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### correct -> uncertainty large\n",
    "loss = bayesian_categorical_crossentropy_nodist(predictions,testY)\n",
    "answers = predictions.argmax(axis=1)\n",
    "w = 0\n",
    "k = 0\n",
    "#### correct_class\n",
    "corclass_uncertainty = {'0':0,'1':0,'2':0,'3':0,'4':0,'5':0,'6':0,'7':0,'8':0,'9':0}\n",
    "nocclass_uncertainty = {'0':0,'1':0,'2':0,'3':0,'4':0,'5':0,'6':0,'7':0,'8':0,'9':0}\n",
    "for i in range(10000):\n",
    "    if answers[i] == testy[i]:\n",
    "        corclass_uncertainty[str(testy[i])] +=loss[i]\n",
    "#         print('correct:')\n",
    "#         print(answers[i])\n",
    "#         print(testy[i])\n",
    "#         print(loss[i])\n",
    "        w = w + loss[i]\n",
    "    else:\n",
    "#         print('not correct:')\n",
    "#         print(answers[i])\n",
    "#         print(testy[i])\n",
    "#         print(loss[i])\n",
    "        nocclass_uncertainty[str(testy[i])] += loss[i]\n",
    "        k = k + loss[i]\n",
    "#     print('------')\n",
    "print('correct uncertainty:')\n",
    "print(w)\n",
    "print('not correct uncertainty:')\n",
    "print(k)\n",
    "print('correct uncertainty each class:')\n",
    "import matplotlib.pyplot as plt\n",
    "x  = corclass_uncertainty.values()\n",
    "x1 = nocclass_uncertainty.values() \n",
    "y  =  np.arange(10)\n",
    "name = category_names('cifar10')\n",
    "w = 0.3\n",
    "ax = plt.subplot(111)\n",
    "ax.bar(y-w, x, width=0.2, color='b', align='center')\n",
    "ax.bar(y,  x1,  width=0.2, color='g', align='center')\n",
    "plt.xticks(y,name)\n",
    "plt.show()\n",
    "\n",
    "# for i in range(10):\n",
    "#     print('class ',i,':')\n",
    "#     print(corclass_uncertainty[str(i)])\n",
    "# print('no correct uncertainty each class:')\n",
    "# for i in range(10):\n",
    "#     print('class ',i,':')\n",
    "#     print(nocclass_uncertainty[str(i)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a CNN - load data\n",
    "from keras.datasets import cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "num_classes = 10\n",
    "epochs = 1600\n",
    "data_augmentation = True\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 48)        13872     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 32, 32, 48)        20784     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 16, 16, 80)        34640     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16, 16, 80)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 16, 16, 80)        57680     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16, 16, 80)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 16, 16, 80)        57680     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16, 16, 80)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 16, 16, 80)        57680     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16, 16, 80)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 16, 16, 80)        57680     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16, 16, 80)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 80)          0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 8, 8, 80)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 8, 8, 128)         92288     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 500)               64500     \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 10)                5010      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,071,542\n",
      "Trainable params: 1,071,542\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalMaxPooling2D\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(48, (3, 3), padding='same',\n",
    "                                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(48, (3, 3), padding='same',\n",
    "                                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(80, (3, 3), padding='same',\n",
    "                                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(80, (3, 3), padding='same',\n",
    "                                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(80, (3, 3), padding='same',\n",
    "                                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(80, (3, 3), padding='same',\n",
    "                                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(80, (3, 3), padding='same',\n",
    "                                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same',\n",
    "                                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128, (3, 3), padding='same',\n",
    "                                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128, (3, 3), padding='same',\n",
    "                                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128, (3, 3), padding='same',\n",
    "                                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128, (3, 3), padding='same',\n",
    "                                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(GlobalMaxPooling2D())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train____________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected activation_22 to have 2 dimensions, but got array with shape (50000, 10, 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-da3328127afe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train____________\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_____________\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected activation_22 to have 2 dimensions, but got array with shape (50000, 10, 10)"
     ]
    }
   ],
   "source": [
    "opt = keras.optimizers.Adam(lr=0.0001)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                            optimizer=opt,\n",
    "                            metrics=['accuracy'])\n",
    "\n",
    "print(\"train____________\")\n",
    "H = model.fit(x_train,y_train,epochs=600,batch_size=128,)\n",
    "print(\"test_____________\")\n",
    "predictions = model.predict(x_test,batch_size = batch_size, )\n",
    "# loss,acc = model.evaluate(X_test,y_test)\n",
    "# print(\"loss=\",loss)\n",
    "# print(\"accuracy=\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
